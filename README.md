


# PySpark ETL Pipeline 

## ðŸ“‹ Project Overview

This PySpark ETL pipeline processes large-scale Amazon sales data to generate comprehensive business insights and KPIs. The pipeline handles data cleaning, transformation, and aggregation with scalable distributed processing.

---

## ðŸŽ¯ Features

### Core Capabilities
- âœ… Distributed data processing with PySpark
- âœ… Comprehensive data quality checks and cleaning
- âœ… 10+ business KPIs calculated automatically
- âœ… Parquet output format for efficient storage
- âœ… HDFS and local filesystem support
- âœ… Optional Kafka integration for real-time processing

### KPIs Calculated

1. **Monthly Revenue** - Revenue trends over time
2. **Profit Margin** - Overall profitability analysis
3. **Region-wise Sales** - Geographic performance breakdown
4. **Average Order Value (AOV)** - Customer spending patterns
5. **Category Performance** - Product category analysis
6. **Fulfillment Performance** - Amazon vs Merchant comparison
7. **B2B vs B2C Analysis** - Customer segment insights
8. **Promotion Impact** - Promotional effectiveness
9. **Top Products** - Best-selling items by revenue
10. **Order Status Distribution** - Order completion rates

---

## ðŸ› ï¸ Installation & Setup

### Prerequisites

```bash
# Required Software
- Python 3.8+
- Apache Spark 3.x
- Java 8 or 11
- Hadoop (optional, for HDFS)
```

### Step 1: Install PySpark

```bash
# Using pip
pip install pyspark

# Or with conda
conda install -c conda-forge pyspark
```

### Step 2: Verify Installation

```bash
# Check Spark installation
pyspark --version

# Test in Python
python3 -c "from pyspark.sql import SparkSession; print('PySpark installed successfully!')"
```

### Step 3: Download the Dataset

Download `Cleaned_Amazon_Sale_Report.csv` from the provided SharePoint link and place it in your project directory.

---

## ðŸš€ Execution Instructions

### Option 1: Local Execution (Recommended for Testing)

```bash
# Navigate to project directory
cd /path/to/project

# Run the pipeline
python pyspark_etl_pipeline.py
```

### Option 2: Spark Submit (Production)

```bash
# Submit to Spark cluster
spark-submit \
  --master local[*] \
  --driver-memory 4g \
  --executor-memory 4g \
  pyspark_etl_pipeline.py
```

### Option 3: HDFS Input/Output

```bash
# First, upload data to HDFS
hdfs dfs -put Cleaned_Amazon_Sale_Report.csv /data/sales/

# Modify the script configuration:
# INPUT_PATH = "hdfs://localhost:9000/data/sales/Cleaned_Amazon_Sale_Report.csv"
# OUTPUT_PATH = "hdfs://localhost:9000/data/output/"

# Run with HDFS paths
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  pyspark_etl_pipeline.py
```

### Option 4: Databricks/EMR

**Databricks:**
1. Upload script to Databricks workspace
2. Create new notebook
3. Import the Python script
4. Run all cells

**AWS EMR:**
```bash
aws emr add-steps \
  --cluster-id j-XXXXXXXXXXXXX \
  --steps Type=Spark,Name="Amazon Sales ETL",\
ActionOnFailure=CONTINUE,\
Args=[--deploy-mode,cluster,--master,yarn,s3://your-bucket/pyspark_etl_pipeline.py]
```

---

## ðŸ“Š Output Structure

After successful execution, you'll find:

```
output/
â”œâ”€â”€ enriched_sales_data/          # Full cleaned dataset
â”‚   â”œâ”€â”€ part-00000.parquet
â”‚   â”œâ”€â”€ part-00001.parquet
â”‚   â””â”€â”€ _SUCCESS
â”œâ”€â”€ kpis/
â”‚   â”œâ”€â”€ monthly_revenue/          # Monthly revenue metrics
â”‚   â”œâ”€â”€ profit_margin/            # Profit analysis
â”‚   â”œâ”€â”€ region_sales/             # Geographic breakdown
â”‚   â”œâ”€â”€ aov/                      # Average order value
â”‚   â”œâ”€â”€ category_performance/     # Category metrics
â”‚   â”œâ”€â”€ fulfillment_performance/  # Fulfillment analysis
â”‚   â”œâ”€â”€ b2b_analysis/             # B2B vs B2C
â”‚   â”œâ”€â”€ promotion_impact/         # Promotion effectiveness
â”‚   â”œâ”€â”€ top_products/             # Best sellers
â”‚   â””â”€â”€ status_distribution/      # Order statuses
â””â”€â”€ summary_report/               # High-level summary
```

---

## Query Results


![1.png](1.png)
 
![2.png](2.png)
 
![3.png](3.png)
 
![4.png](4.png)
 
![5.png](5.png)
 

## ðŸ“ˆ Reading Output Data

### Using PySpark

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("ReadResults").getOrCreate()

# Read enriched data
df = spark.read.parquet("output/enriched_sales_data")
df.show()

# Read specific KPI
monthly_revenue = spark.read.parquet("output/kpis/monthly_revenue")
monthly_revenue.show()
```

### Using Pandas

```python
import pandas as pd

# Read parquet files
df = pd.read_parquet("output/enriched_sales_data")
print(df.head())

# Read KPI
monthly_rev = pd.read_parquet("output/kpis/monthly_revenue")
print(monthly_rev)
```

### Using SQL

```sql
-- In Spark SQL or Hive
CREATE EXTERNAL TABLE enriched_sales
STORED AS PARQUET
LOCATION 'hdfs://output/enriched_sales_data';

SELECT * FROM enriched_sales LIMIT 10;
```

---

## ðŸ”§ Configuration Options

### Modify Script Parameters

Edit these variables in the main script:

```python
# Input/Output paths
INPUT_PATH = "Cleaned_Amazon_Sale_Report.csv"
OUTPUT_PATH = "./output"

# Spark configuration
.config("spark.sql.shuffle.partitions", "200")  # Adjust based on data size
.config("spark.driver.memory", "4g")
.config("spark.executor.memory", "4g")
```

### Performance Tuning

```python
# For large datasets (>10GB)
spark = SparkSession.builder \
    .config("spark.sql.shuffle.partitions", "400") \
    .config("spark.default.parallelism", "400") \
    .config("spark.executor.instances", "10") \
    .config("spark.executor.cores", "4") \
    .config("spark.executor.memory", "8g") \
    .getOrCreate()

# For small datasets (<1GB)
spark = SparkSession.builder \
    .config("spark.sql.shuffle.partitions", "50") \
    .getOrCreate()
```

---

## ðŸ”Œ Optional: Kafka Integration

### Setup Kafka Consumer

```python
# Add to imports
from pyspark.sql.streaming import StreamingQuery

# Kafka configuration
def read_from_kafka(spark, kafka_brokers, topic):
    """Read streaming data from Kafka"""
    df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", kafka_brokers) \
        .option("subscribe", topic) \
        .option("startingOffsets", "latest") \
        .load()
    
    # Parse JSON from Kafka
    from pyspark.sql.functions import from_json, col
    from pyspark.sql.types import StructType, StructField, StringType, DoubleType
    
    schema = StructType([
        StructField("Order_ID", StringType()),
        StructField("Amount", DoubleType()),
        # Add other fields...
    ])
    
    parsed_df = df.select(
        from_json(col("value").cast("string"), schema).alias("data")
    ).select("data.*")
    
    return parsed_df

# Use in pipeline
kafka_df = read_from_kafka(spark, "localhost:9092", "orders")
```

### Run with Kafka

```bash
spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
  pyspark_etl_pipeline.py
```

---

## ðŸ“¸ Expected Console Output

```
============================================================
ðŸš€ PYSPARK ETL PIPELINE - AMAZON SALES ANALYSIS
============================================================
âœ“ Spark Session Created: Amazon_Sales_ETL
âœ“ Spark Version: 3.5.0

============================================================
STEP 1: DATA INGESTION
============================================================
âœ“ Data loaded from: Cleaned_Amazon_Sale_Report.csv
âœ“ Total records: 128,976
âœ“ Total columns: 27

============================================================
STEP 2: DATA CLEANING & TRANSFORMATION
============================================================

ðŸ“Š Initial Record Count: 128,976

ðŸ“‹ Missing Values Analysis:
   - promotion-ids: 45,231 (35.07%)

ðŸ”§ Cleaning Operations:
   âœ“ Removed 234 duplicate orders
   âœ“ Filled missing values
   âœ“ Data type corrections applied

âœ“ Final cleaned records: 128,742

============================================================
STEP 3: FEATURE ENGINEERING
============================================================
âœ“ Added derived columns:
   - Cost, Profit, Profit_Margin_Pct
   - Price_Per_Item
   - Is_B2B, Is_Amazon_Fulfilled, Has_Promotion

============================================================
STEP 4: KPI CALCULATIONS
============================================================

ðŸ“ˆ KPI 1: Monthly Revenue
+----+-----+---------+-------------+-----------+----------------+
|Year|Month|MonthName|Total_Revenue|Order_Count|Total_Units_Sold|
+----+-----+---------+-------------+-----------+----------------+
|2022|   4|      Apr|   15234567.89|     98,432|         145,678|
+----+-----+---------+-------------+-----------+----------------+

... [Additional KPI outputs]

============================================================
âœ… ETL PIPELINE COMPLETED SUCCESSFULLY!
============================================================
```

---

## ðŸ› Troubleshooting

### Common Issues

**1. Memory Errors**
```bash
# Increase driver/executor memory
spark-submit --driver-memory 8g --executor-memory 8g script.py
```

**2. File Not Found**
```python
# Check file path
import os
print(os.path.abspath("Cleaned_Amazon_Sale_Report.csv"))
```

**3. Java Version Issues**
```bash
# Check Java version (needs 8 or 11)
java -version

# Set JAVA_HOME
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk
```

**4. Slow Performance**
```python
# Reduce shuffle partitions for small datasets
.config("spark.sql.shuffle.partitions", "50")
```

---

## ðŸ“ Deliverables Checklist

- âœ… PySpark ETL script (`pyspark_etl_pipeline.py`)
- âœ… Pipeline architecture diagram
- âœ… Complete documentation (this file)
- âœ… Sample outputs in Parquet format
- âœ… Console screenshots showing execution
- âœ… KPI calculation results
- âœ… Data quality report

---

## ðŸŽ“ Learning Resources

- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)
- [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [Performance Tuning](https://spark.apache.org/docs/latest/sql-performance-tuning.html)

---
 